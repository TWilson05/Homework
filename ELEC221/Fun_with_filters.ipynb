{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ef34367-41e5-4c2f-992b-8c5b6f95028c",
      "metadata": {
        "id": "3ef34367-41e5-4c2f-992b-8c5b6f95028c"
      },
      "source": [
        "# Lecture 07. Fun with filters: let's play some music!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605bbee5-325e-4684-a7ec-bb0b8987b143",
      "metadata": {
        "id": "605bbee5-325e-4684-a7ec-bb0b8987b143"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "286b0c40-bc9f-4f64-a800-efb880032cf0",
      "metadata": {
        "id": "286b0c40-bc9f-4f64-a800-efb880032cf0"
      },
      "source": [
        "Please work through this activity together in groups of 3-4. Along the way there will be code blocks to fill in, and conceptual questions to answer. Once you have completed all the sections, you have the **option** to submit (as a group) one copy of this worksheet, and a selected audio sample in PrairieLearn.\n",
        "\n",
        "**This is not a mandatory assignment**. Instead, contributors to each submission will be granted up to 2 bonus points (points, not %) that can be \"banked\" and later applied to any assignment, quiz, or test during the term. To earn the full two points, complete all the functions and answer all the conceptual questions (unless marked optional). Partially-complete submissions will earn 1 point.\n",
        "\n",
        "**Submission deadline**: Sunday at 23:59pm. Late submissions will not be considered for bonus points (but, I will still listen to your work!)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30fcf58a-23b0-44d4-b792-bc1991a4333c",
      "metadata": {
        "id": "30fcf58a-23b0-44d4-b792-bc1991a4333c"
      },
      "source": [
        "Please use the PrairieLearn group work functionality to establish your groups."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10dc3fac-a0ab-4b1a-92a6-4b3e20e0c5ec",
      "metadata": {
        "id": "10dc3fac-a0ab-4b1a-92a6-4b3e20e0c5ec"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f34d3cc-b870-4a3b-90df-908c4f542773",
      "metadata": {
        "id": "6f34d3cc-b870-4a3b-90df-908c4f542773"
      },
      "source": [
        "In Tuesday's lecture, we saw some examples of filters:\n",
        " - frequency-selective filters, such as highpass, lowpass, and bandpass, that eliminate or strongly attenuate certain frequencies\n",
        " - frequency-shaping filters, that can adjust the relative strengths (gain) of different parts of the frequency spectrum\n",
        "\n",
        "A common application of these techniques is in audio processing and equalization. With the right filters, we can boost the bass in a song, remove certain instruments, add distortion, and even change the musical key.\n",
        "\n",
        "All of this is made possible by an algorithm called the fast Fourier *transform* (FFT). This efficient algorithm is at the core of modern-day signal processing, and later in the course we will spend a couple lectures covering it in detail. The goal of this activity is to give you a taste of what you can do with it, and show how the material we're learning in the course can be applied in every day life. After working through this notebook, you'll be able to:\n",
        " - describe how the process of music equalization works from a signals and systems perspective\n",
        " - apply the FFT and related methods to a signal using Python and NumPy\n",
        " - use the FFT to manipulate and modify audio signals\n",
        "\n",
        "The applications of this go beyond just playing with music. You can imagine using a similar process to remove noise from an audio sample in order to make it clearer, remove the sounds of someone typing on a keyboard during a video call, or even (as we will see later in the course) processing other types of signals such as images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60eea8c-3023-4cba-8b62-0d1c8b367bd3",
      "metadata": {
        "id": "a60eea8c-3023-4cba-8b62-0d1c8b367bd3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "from IPython.display import Audio, Image\n",
        "from IPython.core.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2717fd2-c609-4d5e-a5a4-5be55b4fe0a8",
      "metadata": {
        "id": "c2717fd2-c609-4d5e-a5a4-5be55b4fe0a8"
      },
      "source": [
        "## Part 1: import your music"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96e3624-7a9d-4173-8cdb-88ec3b2cffa8",
      "metadata": {
        "id": "f96e3624-7a9d-4173-8cdb-88ec3b2cffa8"
      },
      "source": [
        "Prior to this lecture, I asked you to produce a short clip of your favourite song as a `.wav` file. Below is some code that will should load it up and transform it into a NumPy array. Depending on how you extracted your audio sample (e.g., number of audio channels), you may need to adjust some of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386ea93f-96e8-4870-86ea-df3d5ac48cc0",
      "metadata": {
        "id": "386ea93f-96e8-4870-86ea-df3d5ac48cc0"
      },
      "outputs": [],
      "source": [
        "input_audio = wavfile.read(\"/content/Elec221Song.wav\") # Change your filename here\n",
        "\n",
        "# The result of importing a wavfile should be a tuple of two elements:\n",
        "# the sample rate, and the audio signal, which may be have multiple channels.\n",
        "sample_rate = input_audio[0]\n",
        "\n",
        "# My audio sample had multiple channels. Yours may not.\n",
        "# In my case, I took just one of these channels, and converted it from integer values to floats.\n",
        "channel_0 = input_audio[1][:, 0]\n",
        "audio = np.array(channel_0, dtype=np.float64) / np.max(channel_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1022890-40c3-4fc5-9ce0-e5538cff26ca",
      "metadata": {
        "id": "e1022890-40c3-4fc5-9ce0-e5538cff26ca"
      },
      "source": [
        "If all goes well in the previous cell, you should be able to run the following in order to hear your audio in the Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f313298-f342-428d-8666-803d3bcd5049",
      "metadata": {
        "id": "9f313298-f342-428d-8666-803d3bcd5049"
      },
      "outputs": [],
      "source": [
        "Audio(audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6de59e-6d60-404b-bced-899a82f5fabd",
      "metadata": {
        "id": "ee6de59e-6d60-404b-bced-899a82f5fabd"
      },
      "source": [
        "**Question 1a**: do you think we are working in discrete time, or continuous time here?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71742d8-ce95-4d19-91c5-92a8d7582290",
      "metadata": {
        "id": "c71742d8-ce95-4d19-91c5-92a8d7582290"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8037bdab-2c33-4cad-8089-5ab32c2a1a8f",
      "metadata": {
        "id": "8037bdab-2c33-4cad-8089-5ab32c2a1a8f"
      },
      "source": [
        "**Question 1b**: how does the audio from the `.wav` file sound compared to whatever platform you normally use to listen to music? What do you think has happened to it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ea1ee6-1b41-4aac-838b-2ff9e0fca486",
      "metadata": {
        "id": "33ea1ee6-1b41-4aac-838b-2ff9e0fca486"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "502e84e2-e668-4d6f-9108-43b2585b2c42",
      "metadata": {
        "id": "502e84e2-e668-4d6f-9108-43b2585b2c42"
      },
      "source": [
        "Now, let's plot your audio signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45235688-cf72-4d4e-963b-c9bbea492fae",
      "metadata": {
        "id": "45235688-cf72-4d4e-963b-c9bbea492fae"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "774d597b-641b-486d-9066-d64d58ed0fb4",
      "metadata": {
        "id": "774d597b-641b-486d-9066-d64d58ed0fb4"
      },
      "source": [
        "Most likely, you will see that there is a LOT going on here. But using Fourier analysis, we can pick out and play with the important stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6814442-a243-46b9-bb4c-90b779590f22",
      "metadata": {
        "id": "f6814442-a243-46b9-bb4c-90b779590f22"
      },
      "source": [
        "## Part 2: analyze the spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd63c699-30c1-479f-9bc8-ee2ea929fcc3",
      "metadata": {
        "id": "dd63c699-30c1-479f-9bc8-ee2ea929fcc3"
      },
      "source": [
        "In this cell, we will extract the frequency spectrum (i.e., the Fourier coefficients), of our song. We'll do a few things here that will become clearer next week. First, we will apply a function called `np.fft.rfft`. The `r` stands for real, and we are using it here because our signal is real-valued. Then, we will use a function called `np.fft.rfftfreqs`, which will provide for us the \"real life\" frequency values (in Hz) corresponding to each of the coefficients, rather than integer values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0d0ea1-5d6c-45a6-96ea-3d6ae2e31923",
      "metadata": {
        "id": "1f0d0ea1-5d6c-45a6-96ea-3d6ae2e31923"
      },
      "outputs": [],
      "source": [
        "# A note on normalization: in class, we expressed our regular signal as x[n] = \\sum_k c_k exp[2πjkn/N]\n",
        "# and the Fourier coefficients as c_k = (1/N) \\sum_n x[n] exp[-2πjkn/N]. However, the NumPy FFT\n",
        "# functions have the factor of 1/N on the signal part, rather than on the coefficients. This is\n",
        "# just a difference in definition and convention; in order to match the way we have been doing things\n",
        "# in class, we will set norm=\"forward\".\n",
        "fourier_coefficients = np.fft.rfft(audio, norm=\"forward\")\n",
        "frequency_spectrum =  np.fft.rfftfreq(len(audio), 1 / sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eee9b62-aef4-472d-bc49-fdce882b8f6f",
      "metadata": {
        "id": "1eee9b62-aef4-472d-bc49-fdce882b8f6f"
      },
      "source": [
        "Let's now plot the coefficients. Recall that they are in general complex; we'll plot only the real part for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9932d4b4-c738-46f8-9f4c-c49c23aefc6e",
      "metadata": {
        "id": "9932d4b4-c738-46f8-9f4c-c49c23aefc6e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(frequency_spectrum, fourier_coefficients.real)\n",
        "plt.xlabel(\"Frequency (Hz)\", fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13f6b55-1bec-4859-a7f0-d3fc4f2bdf07",
      "metadata": {
        "id": "a13f6b55-1bec-4859-a7f0-d3fc4f2bdf07"
      },
      "source": [
        "**Question 2a**: In what frequency range does most of the signal lie? Does this make sense?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6674a7b-f88d-4d0e-8462-8427d5d27f9f",
      "metadata": {
        "id": "d6674a7b-f88d-4d0e-8462-8427d5d27f9f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "89d95297-abb2-45a8-82fb-f69273521c94",
      "metadata": {
        "id": "89d95297-abb2-45a8-82fb-f69273521c94"
      },
      "source": [
        "**Question 2b (optional)**: For those of you with musical training, take a closer look at the first ~2000Hz or so of the frequency spectrum. Where do you see peaks, and what does that tell you about the music details of your song?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8e53df-95bf-4746-8405-9232d41bfc46",
      "metadata": {
        "id": "6d8e53df-95bf-4746-8405-9232d41bfc46"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0d15a771-6f5b-44ea-9b20-0ec45e51b8ea",
      "metadata": {
        "id": "0d15a771-6f5b-44ea-9b20-0ec45e51b8ea"
      },
      "source": [
        "## Part 3: frequency-selective filters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0ffaca-25ed-44a0-8e07-33b1e2ddf31e",
      "metadata": {
        "id": "da0ffaca-25ed-44a0-8e07-33b1e2ddf31e"
      },
      "source": [
        "Now that we have our frequency spectrum, we can can start manipulating our music. Let's start with the basics: complete the functions with the appropriate frequency response to implement simple lowpass, highpass, bandpass, and bandstop filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ccb28c5-0b0b-48ca-8748-bb2d057dba21",
      "metadata": {
        "id": "9ccb28c5-0b0b-48ca-8748-bb2d057dba21"
      },
      "outputs": [],
      "source": [
        "def lowpass_filter(frequencies, spectrum, max_freq):\n",
        "    \"\"\"A lowpass filter.\n",
        "\n",
        "    Filter out the parts of the spectrum above the specified frequency.\n",
        "\n",
        "    Args:\n",
        "        frequencies (array[float]): The frequencies of the Fourier coefficients\n",
        "            (in Hz, obtained from np.fft.rfftfreqs).\n",
        "        spectrum (array[complex]): The Fourier coefficients obtained by applying\n",
        "            the FFT to an audio signal.\n",
        "        max_freq (float): The maximum frequency, in Hz, allowed by the lowpass filter.\n",
        "\n",
        "    Returns:\n",
        "        array[complex]: A modified spectrum containing only the Fourier coefficients\n",
        "        up to the specified frequency.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if frequencies[i] < max_freq:\n",
        "        output.append(spectrum[i])\n",
        "      else:\n",
        "        output.append(0)\n",
        "    return output\n",
        "\n",
        "\n",
        "def highpass_filter(frequencies, spectrum, min_freq):\n",
        "    \"\"\"A highpass filter.\n",
        "\n",
        "    Filter out the parts of the spectrum below the specified frequency.\n",
        "\n",
        "    Args:\n",
        "        frequencies (array[float]): The frequencies of the Fourier coefficients\n",
        "            (in Hz, obtained from np.fft.rfftfreqs).\n",
        "        spectrum (array[complex]): The Fourier coefficients obtained by applying\n",
        "            the FFT to an audio signal.\n",
        "        min_freq (float): The minimum frequency, in Hz, allowed by the highpass filter.\n",
        "\n",
        "    Returns:\n",
        "        array[complex]: A modified spectrum containing only the Fourier coefficients\n",
        "        above the specified frequency.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if frequencies[i] > min_freq:\n",
        "        output.append(spectrum[i])\n",
        "      else:\n",
        "        output.append(0)\n",
        "    return output\n",
        "\n",
        "\n",
        "def bandpass_filter(frequencies, spectrum, min_freq, max_freq):\n",
        "    \"\"\"A bandpass filter.\n",
        "\n",
        "    Keep only the parts of the frequency spectrum in a given range.\n",
        "\n",
        "    Args:\n",
        "        frequencies (array[float]): The frequencies of the Fourier coefficients\n",
        "            (in Hz, obtained from np.fft.rfftfreqs).\n",
        "        spectrum (array[complex]): The Fourier coefficients obtained by applying\n",
        "            the FFT to an audio signal.\n",
        "        min_freq (float): The minimum frequency, in Hz, allowed by the bandpass filter.\n",
        "        max_freq (float): The maximum frequency, in Hz, allowed by the bandpass filter.\n",
        "\n",
        "    Returns:\n",
        "        array[complex]: A modified spectrum containing only the Fourier coefficients\n",
        "        within the given frequency band.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if frequencies[i] < max_freq and frequencies[i] > min_freq:\n",
        "        output.append(spectrum[i])\n",
        "      else:\n",
        "        output.append(0)\n",
        "    return output\n",
        "\n",
        "\n",
        "def bandstop_filter(frequencies, spectrum, min_freq, max_freq):\n",
        "    \"\"\"A bandstop filter.\n",
        "\n",
        "    Remove parts of the frequency spectrum in a given range.\n",
        "\n",
        "    Args:\n",
        "        frequencies (array[float]): The frequencies of the Fourier coefficients\n",
        "            (in Hz, obtained from np.fft.rfftfreqs).\n",
        "        spectrum (array[complex]): The Fourier coefficients obtained by applying\n",
        "            the FFT to an audio signal.\n",
        "        min_freq (float): The lowest frequency, in Hz, stopped by the filter.\n",
        "        max_freq (float): The highest frequency, in Hz, stopped by the filter.\n",
        "\n",
        "    Returns:\n",
        "        array[complex]: A modified spectrum containing only the Fourier coefficients\n",
        "        outside the given frequency band.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if not (frequencies[i] < max_freq and frequencies[i] > min_freq):\n",
        "        output.append(spectrum[i])\n",
        "      else:\n",
        "        output.append(0)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283c0135-8630-478d-84af-c1ea08025fe0",
      "metadata": {
        "id": "283c0135-8630-478d-84af-c1ea08025fe0"
      },
      "source": [
        "Try applying some of your filters to your Fourier coefficients. Plot your results to see what happened to the spectrum to make sure they are working as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8381eea2-53a5-48d4-89e5-c84bd82d407d",
      "metadata": {
        "id": "8381eea2-53a5-48d4-89e5-c84bd82d407d"
      },
      "outputs": [],
      "source": [
        "fourier_coefficients = np.fft.rfft(audio, norm=\"forward\")\n",
        "frequency_spectrum =  np.fft.rfftfreq(len(audio), 1 / sample_rate)\n",
        "\n",
        "lowpass_song = lowpass_filter(frequency_spectrum,fourier_coefficients,2500)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(frequency_spectrum,lowpass_song)\n",
        "plt.xlabel(\"Frequency (Hz)\", fontsize=14)\n",
        "\n",
        "Audio(np.fft.irfft(lowpass_song), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e006f299-1560-4eb8-9c54-4d6df3fa523b",
      "metadata": {
        "id": "e006f299-1560-4eb8-9c54-4d6df3fa523b"
      },
      "outputs": [],
      "source": [
        "fourier_coefficients = np.fft.rfft(audio, norm=\"forward\")\n",
        "frequency_spectrum =  np.fft.rfftfreq(len(audio), 1 / sample_rate)\n",
        "\n",
        "highpass_song = highpass_filter(frequency_spectrum,fourier_coefficients,3000)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(frequency_spectrum,highpass_song)\n",
        "plt.xlabel(\"Frequency (Hz)\", fontsize=14)\n",
        "\n",
        "Audio(np.fft.irfft(highpass_song), rate=sample_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourier_coefficients = np.fft.rfft(audio, norm=\"forward\")\n",
        "frequency_spectrum =  np.fft.rfftfreq(len(audio), 1 / sample_rate)\n",
        "\n",
        "bandpass_song = bandpass_filter(frequency_spectrum,fourier_coefficients,1500,2000)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(frequency_spectrum,bandpass_song)\n",
        "plt.xlabel(\"Frequency (Hz)\", fontsize=14)\n",
        "\n",
        "Audio(np.fft.irfft(bandpass_song), rate=sample_rate)"
      ],
      "metadata": {
        "id": "qHMOyjepRzn9"
      },
      "id": "qHMOyjepRzn9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fourier_coefficients = np.fft.rfft(audio, norm=\"forward\")\n",
        "frequency_spectrum =  np.fft.rfftfreq(len(audio), 1 / sample_rate)\n",
        "\n",
        "bandstop_song = bandstop_filter(frequency_spectrum,fourier_coefficients,50,3000)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(frequency_spectrum,bandstop_song)\n",
        "plt.xlabel(\"Frequency (Hz)\", fontsize=14)\n",
        "\n",
        "Audio(np.fft.irfft(bandstop_song), rate=sample_rate)"
      ],
      "metadata": {
        "id": "2nbZZdg_RzdS"
      },
      "id": "2nbZZdg_RzdS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5dcc063e-74bd-4a82-b166-bd65025aace9",
      "metadata": {
        "id": "5dcc063e-74bd-4a82-b166-bd65025aace9"
      },
      "source": [
        "We've computed the Fourier coefficients, and manipulated them in some way. Now, we want to get back to audio. How can we do that? Think about how this would work for the Fourier series as we saw in class, and explore the NumPy documentation to discover how to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f430b5-a46a-444d-8be3-4e21f8f98baf",
      "metadata": {
        "id": "03f430b5-a46a-444d-8be3-4e21f8f98baf"
      },
      "source": [
        "Once you can get back to audio, try and modify your music sample in the following way:\n",
        " - remove the drums (to the best of your ability)\n",
        " - remove only the bass\n",
        " - extract only certain musical notes or chords (e.g., keep only \"C\" and its harmonics)\n",
        " - extract the part of only a certain instrument or set of instruments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc40e7f4-2d34-4d45-af8f-2b98ae20eb5d",
      "metadata": {
        "id": "dc40e7f4-2d34-4d45-af8f-2b98ae20eb5d"
      },
      "outputs": [],
      "source": [
        "noSnare = bandstop_filter(frequency_spectrum,fourier_coefficients,200,250)\n",
        "noDrums = bandstop_filter(frequency_spectrum,noSnare,30,60)\n",
        "\n",
        "Audio(np.fft.irfft(noDrums), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc3c75e-ba0f-45a0-b9ee-6b67a56860c0",
      "metadata": {
        "id": "6bc3c75e-ba0f-45a0-b9ee-6b67a56860c0"
      },
      "outputs": [],
      "source": [
        "noBass = bandstop_filter(frequency_spectrum,fourier_coefficients,60,250)\n",
        "\n",
        "Audio(np.fft.irfft(noBass), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noD = bandstop_filter(frequency_spectrum,fourier_coefficients,293,295)\n",
        "\n",
        "Audio(np.fft.irfft(noD), rate=sample_rate)"
      ],
      "metadata": {
        "id": "ANTmP2UjUQRN"
      },
      "id": "ANTmP2UjUQRN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bea7d5a8-95d4-48f7-a8c3-d22230f8dc4c",
      "metadata": {
        "id": "bea7d5a8-95d4-48f7-a8c3-d22230f8dc4c"
      },
      "source": [
        "**Question 3**: How well did your simple filters work? Which of these tasks was (or do you think would be) the most difficult, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba97b456-f7e3-45bd-b9dc-1cb33461920c",
      "metadata": {
        "id": "ba97b456-f7e3-45bd-b9dc-1cb33461920c"
      },
      "source": [
        "The filters worked very well in reducing unwanted frequencies to zero. These are very idealized filters so they work as expected.\n",
        "\n",
        "The bandstop filter was the most difficult to implement since it removes two different ranges of frequencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d586d6-b77b-472f-a835-5c09bd0fac85",
      "metadata": {
        "id": "34d586d6-b77b-472f-a835-5c09bd0fac85"
      },
      "source": [
        "## Part 4: frequency-shaping filters and equalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05cc039a-fba9-4b6a-a1ee-a860eb4f9028",
      "metadata": {
        "id": "05cc039a-fba9-4b6a-a1ee-a860eb4f9028"
      },
      "source": [
        "You've probably all played with something like [this](https://i0.wp.com/thetechhacker.com/wp-content/uploads/2016/10/FLAC-Player-SD-for-Windows-Phone-8.jpg?fit=472%2C787&ssl=1) before when attempting to improve the quality of sound coming through your computer or headphones. Rather than simply removing or extracting certain frequencies, sometimes we want to adjust the relative strengths of certain frequency bands. This could be for numerous reasons: optimizing how audio sounds in a particular room or physical space, emphasizing certain features depending on the musical genre, or balancing out the relative volumes of different instruments to highlight, e.g., a solo.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd278798-24b2-4192-9117-f79d9ff49eae",
      "metadata": {
        "id": "dd278798-24b2-4192-9117-f79d9ff49eae"
      },
      "source": [
        "Frequency bands are classified as bass, midrange, and high end (treble), often with further subdivisions therein. For example, here are a couple different charts that show this breakdown, along with rough frequency ranges:\n",
        " - https://www.teachmeaudio.com/mixing/techniques/audio-spectrum\n",
        " - https://www.sandburgmusic.org/uploads/4/6/7/1/46719067/editor/audiospectrum_1.gif\n",
        " - https://reference-audio-analyzer.pro/en/hp-fr.php#gsc.tab=0\n",
        " - https://www.masteringthemix.com/blogs/learn/understanding-the-different-frequency-ranges"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "633d55da-cfea-46da-9b02-7ce22e2637bf",
      "metadata": {
        "id": "633d55da-cfea-46da-9b02-7ce22e2637bf"
      },
      "source": [
        "In music equalization, when we seek to change the amplitudes of certain frequency bands, we are adjusting their *relative* volume, rather than an absolute increase or decrease in the amplitude. The unit of adjustment is the decibel (dB):\n",
        "\n",
        "$$\n",
        "\\hbox{dB} = 20 \\log_{10} \\frac{\\hbox{|Amplitude|}}{\\hbox{|Reference amplitude|}}\n",
        "$$\n",
        "\n",
        "Note that the dB scale is logarithmic. For example, if we wanted to increase the strength of a frequency by 2 dB, we would need to rescale it by\n",
        "\n",
        "$$\n",
        "\\hbox{|Amplitude|} \\approx 1.26 \\cdot \\hbox{|Reference amplitude|}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f552e23e-a8e9-435b-b64f-a1b391f6faea",
      "metadata": {
        "id": "f552e23e-a8e9-435b-b64f-a1b391f6faea"
      },
      "source": [
        "First, write a function to boost the bass of your signal by a specified number of dB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2541ac-5231-4d77-ae84-e6821a9d6ff5",
      "metadata": {
        "id": "7e2541ac-5231-4d77-ae84-e6821a9d6ff5"
      },
      "outputs": [],
      "source": [
        "def bassboost(input_audio, bass):\n",
        "    \"\"\"Boost the bass of a given input signal.\n",
        "\n",
        "    This function should do it all - compute the spectrum of the signal,\n",
        "    adjust it as needed, and return the processed signal.\n",
        "\n",
        "    Args:\n",
        "        input_audio (array[float]): The input *audio* signal.\n",
        "        bass (float): The amount, in dB, to boost the bass.\n",
        "\n",
        "    Returns:\n",
        "        array[float]: The output audio signal with boosted bass.\n",
        "    \"\"\"\n",
        "    fourier_coefficients = np.fft.rfft(input_audio, norm=\"forward\")\n",
        "    frequencies =  np.fft.rfftfreq(len(input_audio), 1 / sample_rate)\n",
        "\n",
        "    boost_factor = 10**(bass/20)\n",
        "\n",
        "    boosted = []\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if (frequencies[i] < 250 and frequencies[i] > 30):\n",
        "        boosted.append(boost_factor*fourier_coefficients[i])\n",
        "      else:\n",
        "        boosted.append(fourier_coefficients[i])\n",
        "\n",
        "    return boosted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7358f6e-6a2e-4aef-8385-ed22cb5db21d",
      "metadata": {
        "id": "f7358f6e-6a2e-4aef-8385-ed22cb5db21d"
      },
      "source": [
        "Try applying it to your audio - does it sound like you expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fbc69a-f1db-4c4c-b4a3-3b777bdbcb48",
      "metadata": {
        "id": "d7fbc69a-f1db-4c4c-b4a3-3b777bdbcb48"
      },
      "outputs": [],
      "source": [
        "boosted = bassboost(audio, 10)\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.plot(boosted)\n",
        "Audio(np.fft.irfft(boosted), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb430df4-11c2-4085-8b1c-86c3a5919f11",
      "metadata": {
        "id": "eb430df4-11c2-4085-8b1c-86c3a5919f11"
      },
      "outputs": [],
      "source": [
        "boosted = bassboost(audio, 5)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(boosted)\n",
        "Audio(np.fft.irfft(boosted), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e3038a-b804-4864-8268-51f362c7c003",
      "metadata": {
        "id": "b8e3038a-b804-4864-8268-51f362c7c003"
      },
      "source": [
        "**Question 4a**: After how many decibels of increase do you begin to actually hear a difference in the sound of the music? What do you notice about how it sounds now as a whole?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1b807b-b46b-440b-b5bd-0b2dc06728b5",
      "metadata": {
        "id": "2a1b807b-b46b-440b-b5bd-0b2dc06728b5"
      },
      "source": [
        "After about a 5 dB increase, we start to hear a more pronounced increase in the bass sound. The higher frequencies are not as loud by comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5c402e-a50f-4c2f-89d9-4c56678db6bf",
      "metadata": {
        "id": "0c5c402e-a50f-4c2f-89d9-4c56678db6bf"
      },
      "source": [
        "Finally, write a more sophisticated equalizer that will allow you to adjust the strengths of multiple frequency bands. This is like a very simple programmatic version of an equalizer with multiple knobs or sliders.\n",
        "\n",
        "Here are a few additional resources on audio mixing and equalization that you might find helpful:\n",
        " - https://www.teachmeaudio.com/mixing/techniques/overview\n",
        " - https://ampedstudio.com/equalization/\n",
        " - https://www.digitaltrends.com/home-theater/eq-explainer/\n",
        " - https://mynewmicrophone.com/complete-guide-to-audio-equalization-eq-hardware-software/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8830c65-c53a-4822-a9ef-23cb5d3bf64f",
      "metadata": {
        "id": "f8830c65-c53a-4822-a9ef-23cb5d3bf64f"
      },
      "outputs": [],
      "source": [
        "def equalizer(input_audio, bass, mid, high):\n",
        "    \"\"\"Adjust the strengths of different frequency bands in an audio signal.\n",
        "\n",
        "    You may use the original specifciation of bass, mid, high (and select the\n",
        "    appropriate frequencies), or choose to make this more fine-grained.\n",
        "\n",
        "    This function should do it all - compute the spectrum of the signal,\n",
        "    adjust it as needed, and return the processed signal.\n",
        "\n",
        "    Args:\n",
        "        input_audio (array[float]): The input *audio* signal.\n",
        "        bass (float): The amount, in dB, to adjust the bass frequency band.\n",
        "        mid (float): The amount, in dB, to adjust the mid frequency band.\n",
        "        high (float): The amount, in dB, to adjust the high frequency band.\n",
        "\n",
        "    Returns:\n",
        "        array[float]: The output audio signal with its frequency bands\n",
        "        adjusted by the specified relative amplitudes.\n",
        "    \"\"\"\n",
        "    fourier_coefficients = np.fft.rfft(input_audio, norm=\"forward\")\n",
        "    frequencies =  np.fft.rfftfreq(len(input_audio), 1 / sample_rate)\n",
        "\n",
        "    bass_factor = 10**(bass/20)\n",
        "    mid_factor = 10**(mid/20)\n",
        "    high_factor = 10**(high/20)\n",
        "\n",
        "    equalized = []\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if (frequencies[i] < 250 and frequencies[i] > 1):\n",
        "        equalized.append(bass_factor*fourier_coefficients[i])\n",
        "      elif (frequencies[i] < 2000 and frequencies[i] >= 250):\n",
        "        equalized.append(mid_factor*fourier_coefficients[i])\n",
        "      elif (frequencies[i] >= 2000):\n",
        "        equalized.append(high_factor*fourier_coefficients[i])\n",
        "      else:\n",
        "        equalized.append(fourier_coefficients[i])\n",
        "\n",
        "    return equalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd29f21-9596-479a-a40b-04317fad70e2",
      "metadata": {
        "id": "2bd29f21-9596-479a-a40b-04317fad70e2"
      },
      "outputs": [],
      "source": [
        "equalized = equalizer(audio, 10, -5, 20)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(equalized)\n",
        "Audio(np.fft.irfft(equalized), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19d753f-c44f-4412-baa4-b7c4c9068166",
      "metadata": {
        "id": "c19d753f-c44f-4412-baa4-b7c4c9068166"
      },
      "source": [
        "Try out some different settings on your song!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef63f1f-bfa9-4101-b94e-f68c27d5efea",
      "metadata": {
        "id": "4ef63f1f-bfa9-4101-b94e-f68c27d5efea"
      },
      "outputs": [],
      "source": [
        "equalized = equalizer(audio, -10, 10, -10)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(equalized)\n",
        "Audio(np.fft.irfft(equalized), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ec74cb-4c41-4689-b56e-03f1789dbe57",
      "metadata": {
        "id": "13ec74cb-4c41-4689-b56e-03f1789dbe57"
      },
      "outputs": [],
      "source": [
        "equalized = equalizer(audio, -20, -20, 50)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(equalized)\n",
        "Audio(np.fft.irfft(equalized), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d451de1a-6548-4edf-ac32-c8960599bfdf",
      "metadata": {
        "id": "d451de1a-6548-4edf-ac32-c8960599bfdf"
      },
      "source": [
        "**Question 4b:** Everything we have done so far has been on a relatively long clip of a song after it is loaded in. You might be wondering how all of this works in \"real time\", when you change the equalizer settings on the fly. How do you think audio signals are actually processed in software, e.g., when you're streaming a song?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b379ec-d95f-4c4c-9c49-b2e10bd0e92f",
      "metadata": {
        "id": "83b379ec-d95f-4c4c-9c49-b2e10bd0e92f"
      },
      "source": [
        "There's a loop that checks and updates the equalizer settings each discretized time interval to update the discrete fourier coefficients for that time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c4284d-6e9f-4ba8-bba9-daa67ac105f3",
      "metadata": {
        "id": "a7c4284d-6e9f-4ba8-bba9-daa67ac105f3"
      },
      "source": [
        "## Part 5: additional challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d49af77-c3c9-4f78-89a9-8d615d9a392b",
      "metadata": {
        "id": "1d49af77-c3c9-4f78-89a9-8d615d9a392b"
      },
      "source": [
        "If you (optionally) want to take things further, try doing the following:\n",
        " - changing the musical key of your audio sample\n",
        " - playing with some more sophisticated filters (e.g., adding a slope with attenuation rather than a simple cutoff)\n",
        " - implement some \"preset\" equalization settings for specific genres of music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8c6230-fb85-4f08-9dfb-f525aaca8057",
      "metadata": {
        "id": "4a8c6230-fb85-4f08-9dfb-f525aaca8057"
      },
      "outputs": [],
      "source": [
        "def fancy_pass(input_audio, min_freq, max_freq, fade = False, bandpass = False):\n",
        "  \"\"\" Can be used as either a bandpass or bandstop filter (defaults as bandstop)\n",
        "  but instead of removing a frequency outside the specified range, it slowly\n",
        "  attentuates it from the song. Ie: min_freq = 1000, you will still hear bass at\n",
        "  the beginning but won't hear bass at the end of the song\n",
        "\n",
        "  Args:\n",
        "        input_audio (array[float]): The input *audio* signal.\n",
        "        min_freq (float): Lowest frequency, in Hz, within the pass.\n",
        "        mid (float): Highest frequency, in Hz, within the pass.\n",
        "        fade (boolean): Whether to cut the pass or attentuate it over time, defaults false.\n",
        "        bandpass (boolean): whether to act like a bandpass or bandstop, defaults false (bandstop)\n",
        "\n",
        "    Returns:\n",
        "        array[float]: The output audio signal with its frequency bands\n",
        "        adjusted by the attenuation filter.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  spectrum = np.fft.rfft(input_audio, norm=\"forward\")\n",
        "  frequencies =  np.fft.rfftfreq(len(input_audio), 1 / sample_rate)\n",
        "\n",
        "  output = []\n",
        "  print(spectrum)\n",
        "\n",
        "  if bandpass:\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if frequencies[i] < max_freq and frequencies[i] > min_freq:\n",
        "        output.append(spectrum[i])\n",
        "      else:\n",
        "        output.append(spectrum[i]/(0.1*i))\n",
        "  else:\n",
        "    for i in range (0, len(frequencies)):\n",
        "      if not (frequencies[i] < max_freq and frequencies[i] > min_freq):\n",
        "        output.append(spectrum[i])\n",
        "      else:\n",
        "        output.append(spectrum[i]/(0.0000001*i))\n",
        "\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can hear the bass leaving the song\n",
        "fancy = fancy_pass(audio, 1000, 5000, True, True)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(fancy)\n",
        "Audio(np.fft.irfft(fancy), rate=sample_rate)"
      ],
      "metadata": {
        "id": "ooPyEs9nWv61"
      },
      "id": "ooPyEs9nWv61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can hear the bass leaving the song\n",
        "fancy = fancy_pass(audio, 1, 5000, True, False)\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(fancy)\n",
        "Audio(np.fft.irfft(fancy), rate=sample_rate)"
      ],
      "metadata": {
        "id": "ky9kWIOsXHDL"
      },
      "id": "ky9kWIOsXHDL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}